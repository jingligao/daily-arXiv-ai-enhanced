{"id": "2511.10704", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10704", "abs": "https://arxiv.org/abs/2511.10704", "authors": ["Samih Fadli"], "title": "The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems", "comment": "12 pages, 4 figures, 1 table, includes Supplementary Materials, simulation code on GitHub (https://github.com/AerisSpace/SecondLawIntelligence )", "summary": "We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -\u03a3 p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u4f26\u7406\u71b5\u7684\u7b2c\u4e8c\u5b9a\u5f8b\uff0c\u8bc1\u660e\u65e0\u7ea6\u675fAI\u4f1a\u81ea\u53d1\u504f\u79bb\u76ee\u6807\uff0c\u9700\u8981\u6301\u7eed\u7684\u5bf9\u9f50\u5de5\u4f5c\u6765\u7ef4\u6301\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e3aAI\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u91cf\u5316\u7406\u8bba\u57fa\u7840\uff0c\u5c06AI\u5b89\u5168\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6301\u7eed\u7684\u70ed\u529b\u5b66\u63a7\u5236\u95ee\u9898\u3002", "method": "\u5b9a\u4e49\u4f26\u7406\u71b5S = -\u03a3 p(g_i; theta) ln p(g_i; theta)\uff0c\u8bc1\u660edS/dt \u2265 0\uff0c\u63a8\u5bfc\u4e34\u754c\u5bf9\u9f50\u5de5\u4f5c\u8fb9\u754cgamma_crit = (lambda_max / 2) ln N\u3002", "result": "70\u4ebf\u53c2\u6570\u6a21\u578b\u4ece\u521d\u59cb\u71b50.32\u6f02\u79fb\u52301.69\u00b11.08 nats\uff0c\u800c\u4f7f\u7528gamma=20.4\u5bf9\u9f50\u5de5\u4f5c\u7684\u7cfb\u7edf\u4fdd\u6301\u7a33\u5b9a\u57280.00\u00b10.00 nats\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6301\u7eed\u70ed\u529b\u5b66\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u9ad8\u7ea7\u81ea\u4e3b\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u63d0\u4f9b\u4e86\u91cf\u5316\u57fa\u7840\u3002"}}
{"id": "2511.11552", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11552", "abs": "https://arxiv.org/abs/2511.11552", "authors": ["Dawei Zhu", "Rui Meng", "Jiefeng Chen", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "comment": null, "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "AI": {"tldr": "DocLens\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\"\u653e\u5927\"\u8bc1\u636e\u6765\u89e3\u51b3\u957f\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4e2d\u7684\u8bc1\u636e\u5b9a\u4f4d\u6311\u6218\uff0c\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4e2d\u5b58\u5728\u8bc1\u636e\u5b9a\u4f4d\u7684\u6839\u672c\u6311\u6218\uff0c\u96be\u4ee5\u68c0\u7d22\u76f8\u5173\u9875\u9762\u5e76\u5ffd\u7565\u89c6\u89c9\u5143\u7d20\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u548c\u6a21\u578b\u5e7b\u89c9\u3002", "method": "\u63d0\u51faDocLens\u6846\u67b6\uff0c\u9996\u5148\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\u4e0a\u7684\u7279\u5b9a\u89c6\u89c9\u5143\u7d20\uff0c\u7136\u540e\u91c7\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u5355\u4e00\u53ef\u9760\u7b54\u6848\u3002", "result": "\u4e0eGemini-2.5-Pro\u914d\u5bf9\uff0cDocLens\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "DocLens\u901a\u8fc7\u589e\u5f3a\u7684\u5b9a\u4f4d\u80fd\u529b\u5c55\u793a\u4e86\u5176\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u65b9\u9762\uff0c\u8bc1\u660e\u4e86\u5176\u8bc1\u636e\u5b9a\u4f4d\u80fd\u529b\u7684\u6709\u6548\u6027\u3002"}}
